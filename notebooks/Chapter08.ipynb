{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5312cefe-8997-4cfc-971d-1c2e859eead1",
   "metadata": {},
   "source": [
    "## 8.1, Least squares\n",
    "\n",
    "> The folder `ElectionsEconomy` contains the data for the example in Section\n",
    "> 7.1.  Load these data, type in the R function `rss()` from page 104, and\n",
    "> evaluate it at several different values of $(a, b)$. Make two graphs: a plot\n",
    "> of the sum of squares of residuals as a function of $a$, with $b$ fixed at its\n",
    "> least squares estimate given in Section 7.1, and a plot of the sum of squares\n",
    "> of residuals as a function of $b$, with $a$ fixed at its least squares\n",
    "> estimate. Confirm that the residual sum of squares is indeed minimized at the\n",
    "> least squares estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e69c0-201a-418a-a291-6ce7de78fcf2",
   "metadata": {},
   "source": [
    "## 8.2, Maximum likelihood\n",
    "\n",
    "> Repeat the previous exercise but this time write a function, similar to\n",
    "> `rss()` on page 104, that computes the logarithm of the likelihood (8.6) as a\n",
    "> function of the data and the parameters $a, b, \\sigma$. Evaluate this function\n",
    "> as several values of these parameters, and make a plot demonstrating that it\n",
    "> is maximized at the values computed from the formulas in the text (with\n",
    "> $\\sigma$ computed using $\\frac{1}{n}$, not $\\frac{1}{n-2}$ see page 104)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71472b7e-cdd6-4c43-b405-a4fc249d2a0c",
   "metadata": {},
   "source": [
    "## 8.3, Least absolute deviation\n",
    "\n",
    "> Repeat 8.1, but instead of calculating and minimizing the sum of squares of\n",
    "> residuals, do this for the sum of absolute values of residuals. Find the\n",
    "> $(a, b)$ that minimizes the sum of absolute values of residuals, and plot the\n",
    "> sum of absolute values of residuals as a function of $a$ and of $b$.\n",
    "> Compare the least squares and least absolute deviation estimates of $(a, b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad313dc-c779-4f18-a896-50859db8dc14",
   "metadata": {},
   "source": [
    "## 8.4, Least squares and least absolute deviation\n",
    "\n",
    "> Construct a set of data $(x, y)_i$, $i = 1, \\ldots, n$, for which the least\n",
    "> squares and least absolute deviation (see Exercise 8.3) estimates of $(a, b)$\n",
    "> in the fit, $y = a + bx$, are much different. What did you have to do to make\n",
    "> this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730460a4-93aa-4dbd-bda4-45cfd70f19a6",
   "metadata": {},
   "source": [
    "## 8.5, Influence of individual data points\n",
    "\n",
    "> A linear regression is fit to the data below. Which point has the most\n",
    "> influence (see Section 8.2) on the slope?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3a15c-61bf-4cdd-971d-e04971ff3bed",
   "metadata": {},
   "source": [
    "## 8.6, Influence of individual data points\n",
    "\n",
    "> (a) Using expression (8.3), compute the influence of each of the data points\n",
    ">     in the election forecasting example on the fitted slope of the model. Make\n",
    ">     a graph plotting influence of point $i$ vs. $x_i$.\n",
    ">\n",
    "> (b) Re-fit the model $n$ times, for each data point $i$ adding 1 to $y_i$.\n",
    ">     Save $\\hat{b}$ from each of these altered regressions, compare to the\n",
    ">     $\\hat{b}$ from the original data, and check that the influence is\n",
    ">     approximately the same as computed above using the formula. (The two\n",
    ">     calculations will not give identical results because `stan_glm` uses a\n",
    ">     prior distribution and so it does not exactly yield the least squares\n",
    ">     estimate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a1f80-d940-4416-8dd8-4dc0a8264633",
   "metadata": {},
   "source": [
    "## 8.7, Least squares slope as a weighted average of individual slopes\n",
    "\n",
    "> (a) Prove that the weighted average slope defined in equation (8.8) is\n",
    ">     equivalent to the least squares regression slope in equation (8.3).\n",
    ">\n",
    "> (b) Demonstrate how this works in a simple case with three data points,\n",
    ">     $(x, y) = (0, 0), (4, 1), (5, 5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec52c2a2-cb73-492d-b9fd-6a933ec5ed8d",
   "metadata": {},
   "source": [
    "## 8.8, Comparing `lm` and `stan_glm`\n",
    "\n",
    "> Use simulated data to compare least squares estimation to default Bayesian\n",
    "> regression:\n",
    ">\n",
    "> (a) Simulate 100 data points from the model, $y = 2 + 3x + \\text{error}$, with\n",
    ">     predictors $x$ drawn from a uniform distribution from 0 to 20, and with\n",
    ">     independent errors drawn from the normal distribution with mean 0 and\n",
    ">     standard deviation 5. Fit the regression of $y$ on $x$ data using `lm`\n",
    ">     and `stan_glm` (using its default settings) and check that the two\n",
    ">     programs give nearly identical results.\n",
    ">\n",
    "> (b) Plot the simulated data and the two fitted regression lines.\n",
    ">\n",
    "> (c) Repeat the two steps above, but try to create conditions for your\n",
    ">     simulation so that `lm` and `stan_glm` give much different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5407f0-fcba-4afb-ac25-ddb1a4cc79d0",
   "metadata": {},
   "source": [
    "## 8.9, Leave-one-out cross validation\n",
    "\n",
    "> As discussed in the context of (8.5), the root mean square of residuals,\n",
    "> $\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i - (\\hat{a} + \\hat{b}x_i))^2}$, is an\n",
    "> underestimate of the error standard deviation $\\sigma$ of the regression\n",
    "> model, because of overfitting, that the parameters $a$ and $b$ are estimated\n",
    "> from the same $n$ data points as are being used to compute the residuals.\n",
    "> Cross validation, which we discuss in detail in Section 11.8, is an\n",
    "> alternative approach to assessing predictive error that avoids some of the\n",
    "> problems of overfitting. The simplest version of cross validation is the\n",
    "> leave-one-out approach, in which the model is fit n times, in each case\n",
    "> excluding one data point, fitting the model to the remaining $n - 1$ data\n",
    "> points, and using this fitted model to predict the held-out observation:\n",
    "> \n",
    "> * For $i = 1, \\dots, n$:\n",
    ">    * Fit the model $y = a + bx + \\text{error}$ to the $n - 1$ data points\n",
    ">      $(x, y)_j, j \\ne i$. Label the estimated regression coefficients as \n",
    ">      $\\hat{a}_{-i}$, $\\hat{b}_{-i}$.\n",
    ">    * Compute the cross-validated residual,\n",
    ">      $r_i^{\\text{CV}} = y_i - (\\hat{a}_{-i} + \\hat{b}_{-i}x_i)$.\n",
    "> * Compute the estimate\n",
    ">    $\\hat{\\sigma}^{\\text{CV}} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^nr_i^2}$.\n",
    ">\n",
    "> (a) Perform the above steps for the elections model from Section 7.1. Compare\n",
    "> three estimates of $\\sigma$: (i) the estimate produced by `stan_glm`,\n",
    "> (ii) formula (8.5), and (iii) $\\hat{\\sigma}^{\\text{CV}}$ as defined above.\n",
    ">\n",
    "> (b) Discuss any differences between the three estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1026231-700c-4367-b498-51be9cf4e5a3",
   "metadata": {},
   "source": [
    "## 8.10 Leave-one-out cross validation\n",
    "\n",
    "> Create a fake dataset $(x, y)_i, i = 1, \\dots, n$, in such a way that there\n",
    "> is a big difference between $\\hat{\\sigma}^{\\text{CV}}$ as defined in the\n",
    "> previous exercise, and the estimated residual standard deviation from (8.5).\n",
    "> Explain what you did to create this discrepancy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
